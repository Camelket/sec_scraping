{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TrainingBERT on RE:\n",
    "1) extract all FrontPages from 100 S-3 filings\n",
    "2) annotate gold examples\n",
    "3) split 80/20\n",
    "4) train\n",
    "PreTraining BERT:\n",
    "1) collect 1000 filings (take S-3, 8-k with relevant items), strip TitlePage\n",
    "2) preTrain BERT with Match the Blanks\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "root_path = Path(r\"C:\\Users\\Olivi\\Desktop\\test_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing annotation process on currently available test set s-3\n",
    "\n",
    "# parse files with s-3 parser and extract Summary and CoverPage from the the filings\n",
    "# save to unannoted_s3_cover_page.txt and unannoted_s3_summary.txt\n",
    "from pathlib import Path\n",
    "from main.parser.filing_parsers import create_htm_filing\n",
    "import re\n",
    "import logging\n",
    "logging.disable(level=logging.DEBUG)\n",
    "fps = list(Path(r\"C:\\Users\\Olivi\\Desktop\\test_set\\set_s3\\filings\").rglob(\"*.htm\"))\n",
    "failed = []\n",
    "for idx, fp in enumerate(fps):\n",
    "    if idx%10 == 0:\n",
    "        print(f\"{idx}|{len(fps)}\")\n",
    "    info = {\n",
    "        \"form_type\":\"S-3\",\n",
    "        \"extension\": \".htm\",\n",
    "        \"path\": fp,\n",
    "        \"filing_date\": \"-\",\n",
    "        \"accession_number\": \"-\",\n",
    "        \"cik\": \"-\",\n",
    "        \"file_number\": \"-\",\n",
    "    }\n",
    "    try:\n",
    "        filings = create_htm_filing(**info)\n",
    "    except AttributeError as e:\n",
    "        failed.append((fp, e))\n",
    "    else:\n",
    "        if isinstance(filings, list):\n",
    "            pass\n",
    "        else:\n",
    "            filings = [filings]\n",
    "        for filing in filings:\n",
    "            sections = filing.get_sections(identifier=re.compile(\"description\"))\n",
    "            if sections:\n",
    "                for section in sections:\n",
    "                    with open(r\"C:\\Users\\Olivi\\Desktop\\test_set\\unannoted_s3_descriptions.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "                        text = section.text_only\n",
    "                        f.write(\"\\n+++::::\")\n",
    "                        f.write(text)\n",
    "\n",
    "            # cover_page = filing.get_section(identifier=re.compile(\"cover\"))\n",
    "            # if cover_page != []:\n",
    "            #     with open(r\"C:\\Users\\Olivi\\Desktop\\test_set\\unannoted_s3_cover_page.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "            #         cover_text = cover_page.text_only\n",
    "            #         f.write(\"\\n+++::::\")\n",
    "            #         f.write(cover_text)\n",
    "            # summary = filing.get_section(identifier=re.compile(\"summary\"))\n",
    "            # if summary != []:\n",
    "            #     with open(r\"C:\\Users\\Olivi\\Desktop\\test_set\\unannoted_s3_summary.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "            #         summary_text = summary.text_only\n",
    "            #         f.write(\"\\n+++::::\")\n",
    "            #         f.write(summary_text)\n",
    "logging.disable(logging.NOTSET)\n",
    "\n",
    "for fail in failed:\n",
    "    print(fail)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use current SpacyFilingTextSearch to add SECU, SECUQUANTITY, CONTRACT entities aswell as the regular ones: PER, ORG, ect..\n",
    "# load and split by \"+++:::\" to get individual pages\n",
    "with open(root_path / \"unannoted_s3_descriptions.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    pages = f.read().split(\"\\n+++::::\")\n",
    "    pages = list(filter(lambda x: len(x)>50, pages))\n",
    "\n",
    "# load pipeline\n",
    "from main.nlp.filing_nlp import SpacyFilingTextSearch\n",
    "import json\n",
    "logger = logging.getLogger(\"filing_nlp\")\n",
    "logger.setLevel(logging.INFO)\n",
    "save_folder = root_path / \"s3_doccano_descriptions\"\n",
    "if not save_folder.exists():\n",
    "    save_folder.mkdir()\n",
    "search = SpacyFilingTextSearch()\n",
    "for i, page in enumerate(pages):\n",
    "    try:\n",
    "        doc = search.nlp(page)\n",
    "        # convert to compatible json format for doccano\n",
    "        # get json first\n",
    "        j = doc.to_json()\n",
    "        doccano_dict = {\"text\": j[\"text\"], \"ents\": [[i[\"start\"], i[\"end\"], i[\"label\"]] for i in j[\"ents\"]]}\n",
    "        save_path = save_folder / f\"{i}.json\"\n",
    "        with open(save_path, \"w\", encoding=\"utf-8\") as g:\n",
    "            json.dump(doccano_dict, g)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Olivi\\Testing\\sec_scraping\\main\\nlp\\filing_nlp_alias_setter.py:1013: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
      "  similarity = origin_token.similarity(target_token)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/176\n",
      "3/176\n",
      "4/176\n",
      "5/176\n",
      "6/176\n",
      "7/176\n",
      "8/176\n",
      "9/176\n",
      "[Security, with, offered, Warrants, of, consequences, States]\n",
      "[States, considerations, discussed, in, Supplement]\n",
      "[Agreement, of, copy, file, with, authorities, in, Canada]\n",
      "[Agreement, of, copy, file, with, authorities, in, States]\n",
      "[Conditions, of, satisfaction, upon, made, entitlement, satisfied, Conditions]\n",
      "[Conditions, of, satisfaction, upon, made, entitlement, of, Company]\n",
      "[Conditions, of, satisfaction, upon, made, entitlement, issue, Company]\n",
      "[Conditions, of, satisfaction, upon, made, entitlement, issue, Company]\n",
      "[Conditions, of, satisfaction, upon, made, provisions, as, to, modification, of, Agreement]\n",
      "[Conditions, of, satisfaction, upon, made, provisions, as, to, modification, of, Agreement, rights, of, Receipts]\n",
      "[Conditions, of, satisfaction, upon, made, provisions, as, to, modification, apply, Company]\n",
      "[Conditions, of, satisfaction, upon, made, provisions, as, to, consequences, tax, U.S.]\n",
      "[Conditions, of, satisfaction, upon, release, entitlement, satisfied, Conditions]\n",
      "[Conditions, of, satisfaction, upon, release, entitlement, of, Company]\n",
      "[Conditions, of, satisfaction, upon, release, entitlement, issue, Company]\n",
      "[Conditions, of, satisfaction, upon, release, entitlement, issue, Company]\n",
      "[Receipts, of, sale, with, connection, in, fees, of, portion, of, payment, in, release, entitlement, satisfied, Conditions]\n",
      "[Receipts, of, sale, with, connection, in, fees, of, portion, of, payment, in, release, entitlement, of, Company]\n",
      "[Receipts, of, sale, with, connection, in, fees, of, portion, of, payment, in, release, entitlement, issue, Company]\n",
      "[Receipts, of, sale, with, connection, in, fees, of, portion, of, payment, in, release, entitlement, issue, Company]\n",
      "[Conditions, satisfied, entitlement, of, Company]\n",
      "[Company, of, entitlement, purchase, by, agreement]\n",
      "[Company, of, entitlement, issue, Company]\n",
      "[Company, of, entitlement, issue, Company]\n",
      "[Company, of, entitlement, include, provisions, as, to, modification, of, Agreement]\n",
      "[Company, of, entitlement, include, provisions, as, to, modification, of, Agreement, rights, of, Receipts]\n",
      "[Company, of, entitlement, include, provisions, as, to, modification, apply, Company]\n",
      "[Company, of, entitlement, include, provisions, as, to, consequences, tax, U.S.]\n",
      "[Agreement, rights, of, Receipts]\n",
      "[Agreement, of, modification, apply, Company]\n",
      "[Agreement, of, consequences, tax, U.S.]\n",
      "[Agreement, of, consequences, tax, U.S., Canadian]\n",
      "[Receipts, of, rights, Agreement, of, modification, apply, Company]\n",
      "[Receipts, of, rights, Agreement, of, consequences, tax, U.S.]\n",
      "[Receipts, of, rights, Agreement, of, consequences, tax, U.S., Canadian]\n",
      "[Company, apply, consequences, tax, U.S.]\n",
      "[agreement, date, apply, Company]\n",
      "[agreement, consequences, tax, U.S.]\n",
      "[agreement, consequences, tax, U.S., Canadian]\n",
      "[Company, apply, consequences, tax, U.S.]\n",
      "[September, at, As, had, Common Shares, 105,667,125]\n",
      "[TSX, on, traded, under, symbol, TMQ]\n",
      "[September, On, was, price, on, TSX]\n",
      "[September, On, was, American, NYSE]\n",
      "[September, On, was, American]\n",
      "[TSX, on, price, was, American, NYSE]\n",
      "[TSX, on, price, was, American]\n",
      "[September, at, As, had, Common Shares, 105,667,125]\n",
      "[TSX, on, traded, under, symbol, TMQ]\n",
      "[November, On, was, price, on, TSX]\n",
      "[November, On, was, American, NYSE]\n",
      "[November, On, was, American]\n",
      "[TSX, on, price, was, American, NYSE]\n",
      "[TSX, on, price, was, American]\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import DocBin\n",
    "from pathlib import Path\n",
    "from sdp_utils import *\n",
    "from main.nlp.filing_nlp import SpacyFilingTextSearch\n",
    "import logging\n",
    "logger = logging.getLogger(\"filing_nlp\")\n",
    "logger.setLevel(logging.INFO)\n",
    "search = SpacyFilingTextSearch()\n",
    "\n",
    "if not Path(r\"C:\\Users\\Olivi\\Desktop\\test_set\\s3_descriptions.spacy\").exists():\n",
    "    with open(r\"C:\\Users\\Olivi\\Desktop\\test_set\\unannoted_s3_descriptions.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        pages = f.read().split(\"\\n+++::::\")\n",
    "        pages = list(filter(lambda x: len(x)>50, pages))\n",
    "\n",
    "    docs = []\n",
    "    for idx, page in enumerate(pages[:10]):\n",
    "        if idx%10:\n",
    "            print(f\"{idx}/{len(pages)}\")\n",
    "        doc = search.nlp(page)\n",
    "        docs.append(doc)\n",
    "    # import pickle\n",
    "    # from spacy.tokens import Span, Token\n",
    "    # for idx, page in enumerate(pages[:2]):\n",
    "    #     if idx%10:\n",
    "    #         print(f\"{idx}/{len(pages)}\")\n",
    "    #     doc = search.nlp(page)\n",
    "#         with open(r\"C:\\Users\\Olivi\\Desktop\\test_set\\desc_test_pickle.txt\", \"wb\") as f:\n",
    "#             doc._.secu_objects = None\n",
    "#             doc._.secuquantity_spans = None\n",
    "#             doc._.secu_objects_map = None\n",
    "#             doc._.quantity_relation_map = None\n",
    "#             doc._.secuquantity_secu_map= None\n",
    "#             doc._.source_quantity_relation_map= None\n",
    "#             keys_to_remove = []\n",
    "#             for key in doc.user_data:\n",
    "#                 if len(key) >= 2:\n",
    "#                     if key[1] == 'source_span_unmerged':\n",
    "#                         keys_to_remove.append(key)\n",
    "#             for key in keys_to_remove:\n",
    "#                 doc.user_data.pop(key)\n",
    "#             pickle.dump(doc, f)\n",
    "#         docs.append(doc)\n",
    "#     docbin = DocBin(docs=docs, store_user_data=True)\n",
    "#     docbin.to_disk(r\"C:\\Users\\Olivi\\Desktop\\test_set\\s3_descriptions.spacy\")\n",
    "# else:\n",
    "#     docbin = DocBin().from_disk(path=r\"C:\\Users\\Olivi\\Desktop\\test_set\\s3_descriptions.spacy\")\n",
    "#     docs = docbin.get_docs(search.nlp.vocab)\n",
    "\n",
    "\n",
    "nvu_sdps = get_sdps_noun_verbal(docs)\n",
    "relations = set()\n",
    "for left, right, sdp_grouped, sdp in nvu_sdps:\n",
    "    info = create_sdp_noun_verbal_dict(left, right, sdp_grouped, sdp)\n",
    "    if info[\"relation\"] is not None:\n",
    "        relations.add(info[\"relation\"])\n",
    "    # print(info)\n",
    "    print([i[\"token\"] for i in info[\"sdp\"]])\n",
    "    # --> sdps arent the way to go ... :#)\n",
    "# print(len(relations),\"/\", len(nvu_sdps))\n",
    "# for rel in relations:\n",
    "#     print(rel)\n",
    "\n",
    "# can i hook into msgpack and register my own encoder/decoder for AliasCache and SECUObject ?\n",
    "# last i could pickle but that would mean way larger files ... \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sec_scraping_testing-er4NC4pi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6071678dbb69e107a06c616957bae4c441d4d322577d1f6af58eab6659c9cd7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
