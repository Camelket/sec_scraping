{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.nlp.filing_nlp import SpacyFilingTextSearch\n",
    "from main.nlp.filing_nlp_SECU_object import QuantityRelation, SourceContext, SourceQuantityRelation\n",
    "from main.parser.filing_parsers import HTMFilingParser\n",
    "import logging\n",
    "import pandas as pd\n",
    "import logging\n",
    "from spacy.tokens import Token, Span, Doc\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SECU_objects_from_text(text: str):\n",
    "        search = SpacyFilingTextSearch()\n",
    "        doc = search.nlp(text)\n",
    "        secu_objects = search.get_SECU_objects(doc)\n",
    "        return secu_objects\n",
    "        \n",
    "def sample_for_SECU_objects(paths):\n",
    "        parser = HTMFilingParser()\n",
    "        search = SpacyFilingTextSearch()\n",
    "        # create logger file, log file name and log sSECU to info\n",
    "        exercise_date = []\n",
    "        exercise_price = []\n",
    "        expiry = []\n",
    "        secu_keys = []\n",
    "        quants = []\n",
    "        secus = []\n",
    "        for path in paths:\n",
    "            text = parser.clean_text_only_filing(parser.get_doc(path))\n",
    "            doc = search.nlp(text)\n",
    "            secu_objects = doc._.secu_objects\n",
    "            secus.append(secu_objects)\n",
    "            for secu_key, values in secu_objects.items():\n",
    "                secu_keys.append(secu_key)\n",
    "                has_exercise_price, has_expiry, has_exercise_date, has_quant = 0, 0, 0, 0\n",
    "                for secu in values:\n",
    "                    if secu.exercise_price:\n",
    "                        has_exercise_price += 1\n",
    "                    if secu.expiry_date:\n",
    "                        has_expiry += 1\n",
    "                    if secu.exercise_date:\n",
    "                        has_exercise_date += 1\n",
    "                    if secu.quantity_relations != []:\n",
    "                        has_quant += len(secu.quantity_relations)\n",
    "                exercise_date.append(has_exercise_date)\n",
    "                exercise_price.append(has_exercise_price)\n",
    "                expiry.append(has_expiry)\n",
    "                quants.append(has_quant)\n",
    "        df = pd.DataFrame(data={\"name\": secu_keys, \"quantity_relations\": quants ,\"exercise_date\": exercise_date, \"exercise_price\": exercise_price, \"expiry\": expiry})\n",
    "        return secus, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.disable(logging.INFO)\n",
    "filing_paths = [\n",
    "    r\"C:/Users/Olivi/Testing/sec_scraping/tests/test_resources/filings/0000831547/S-3/000083154720000018/cleans-3.htm\",\n",
    "    # r\"C:/Users/Olivi/Testing/sec_scraping/tests/test_resources/filings/0001325879/S-3/000119312518218817/d439397ds3.htm\",\n",
    "    r\"C:/Users/Olivi/Testing/sec_scraping/tests/test_resources/filings/0001453593/S-3/000149315221008120/forms-3.htm\"\n",
    "]\n",
    "secus = []\n",
    "parser = HTMFilingParser()\n",
    "securities, df = sample_for_SECU_objects(filing_paths)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.nlp.filing_nlp_SECU_object import SECU\n",
    "def get_outstanding_relations(self: SECU):\n",
    "    if self.amods:\n",
    "        str_secu_amods = [i.lower_ for i in self.amods]\n",
    "    else:\n",
    "        str_secu_amods = []\n",
    "    for qr in self.quantity_relations:\n",
    "        quant = qr.quantity\n",
    "        if quant.datetime_relation:\n",
    "            if quant.original._.negated is False:\n",
    "                if quant.amods is not None:\n",
    "                    amods = [i.lower_ for i in quant.amods] + str_secu_amods\n",
    "                else:\n",
    "                    amods = str_secu_amods\n",
    "\n",
    "                if (\"outstanding\" in amods) or (\"outstanding\" in str_secu_amods):\n",
    "                    certainty = 1.0\n",
    "                    if quant.parent_verb:\n",
    "                        if quant.parent_verb._.certainty_info:\n",
    "                            certainty = quant.parent_verb._.certainty_info.determine_level()\n",
    "                        if certainty > 0.85:\n",
    "                            if quant.amount.amount is not None:\n",
    "                                print(self.secu_key)\n",
    "                                print(quant.original.sent)\n",
    "                                print(amods, \" \", quant.amount, f\" [{'negative' if quant.parent_verb._.negated is True else 'positive'}]{quant.parent_verb}@{certainty}\")\n",
    "                            else:\n",
    "                                print(\"GOT NONE AMOUNT, DEBUG:\")\n",
    "                                print(quant.original, quant.original.sent)\n",
    "                                for token in quant.original.sent:\n",
    "                                    print(token, token.dep_, token.ent_type_)\n",
    "                                print(amods, \" \", quant.amount, f\" [{'negative' if quant.parent_verb._.negated is True else 'positive'}]{quant.parent_verb}@{certainty}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.nlp.filing_nlp import token_adj_getter\n",
    "no_amods_count = 0\n",
    "for d in securities:\n",
    "    for secu_key, secus in d.items():\n",
    "        for secu in secus:\n",
    "            # figure out why it misses some outstanding\n",
    "            outstanding = secu.get_outstanding_quantity_relations()\n",
    "            if outstanding:\n",
    "                for qr in outstanding:\n",
    "                    print(qr)\n",
    "            # if secu.quantity_relations:\n",
    "            #     for qr in secu.quantity_relations:\n",
    "            #         quant = qr.quantity\n",
    "            #         certainty = 1.0\n",
    "            #         if quant.parent_verb:\n",
    "            #             if quant.parent_verb._.certainty_info:\n",
    "            #                 certainty = quant.parent_verb._.certainty_info.determine_level()\n",
    "            #         if quant.amods is not None:\n",
    "            #             pass\n",
    "            #             # print(quant.amods, \" \", quant.amount, f\"[{'negative' if quant.parent_verb._.negated is True else 'positive'}]{quant.parent_verb}@{certainty}\")\n",
    "            #         else:\n",
    "            #             no_amods_count += 1\n",
    "            #             # print(quant.amods, secu.amods, \" \", quant.amount, f\"[{'negative' if quant.parent_verb._.negated is True else 'positive'}]{quant.parent_verb}@{certainty}\")\n",
    "            #             # print(quant.original.sent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.nlp.filing_nlp import SpacyFilingTextSearch\n",
    "text = \"On March 31, 2021, we had 10000 shares of common stock outstanding.\"\n",
    "# text = \"As of March 31, 2021, we had outstanding warrants to purchase 7,532,390 shares of our common stock, and stock options to purchase 2,176,272 shares of our common stock.\"\n",
    "# text = \"As of April 20, 2022, we had warrants outstanding to issue 20000 shares of common stock at 3.0 $ per share.\"\n",
    "# text = \"On April 20, 2022, we sold 10000 shares of our common stock at a purchase price of 2 $ per share. On April 20, 2022, we sold 10000 shares of our common stock at a purchase price of 2$ per share.\"\n",
    "search = SpacyFilingTextSearch()\n",
    "doc = search.nlp(text)\n",
    "for secu_key, secus in doc._.secu_objects.items():\n",
    "    for secu in secus:\n",
    "        print(secu.secu_key, \"_______\")\n",
    "        qr = secu.quantity_relations[0]\n",
    "        print(qr.quantity.parent_verb, qr.quantity.amods)\n",
    "        outstanding = secu.get_outstanding_quantity_relations()\n",
    "        if len(secu.source_quantity_relations) > 0:\n",
    "            print(secu.source_quantity_relations[0])    \n",
    "            print(secu.source_quantity_relations[0].target_secu)\n",
    "        if outstanding:\n",
    "            for qr in outstanding:\n",
    "                print(qr)\n",
    "        print(secu.attr_matcher._get_at_context(secu.parent_verb))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.nlp.filing_nlp_dependency_matcher import SecurityDependencyAttributeMatcher\n",
    "attr_matcher = SecurityDependencyAttributeMatcher()\n",
    "secus = doc._.secu_objects\n",
    "for secu_key, vals in secus.items():\n",
    "    for secu in vals:\n",
    "        print(secu.secu_key, secu.original.i)\n",
    "        print(secu.source_quantity_relations)\n",
    "        print(secu.quantity_relations)\n",
    "        # print(\"source: \", attr_matcher.get_possible_source_quantities(secu.original))\n",
    "        # print(\"real: \", attr_matcher.get_quantities(secu.original))\n",
    "        print(\"--------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD ONLY TEXT OF A FILING (WITH COMMON UNICODE REPLACED, BUT NEWLINES PRESERVED)\n",
    "from main.parser.filing_parsers import HTMFilingParser\n",
    "from pathlib import Path\n",
    "parser = HTMFilingParser()\n",
    "file_paths = [i for i in Path(r\"C:\\Users\\Olivi\\Desktop\\test_set\\filings\\0001309082\\S-3\").rglob(\"*.htm\")]\n",
    "text_only = []\n",
    "for p in file_paths:\n",
    "    text = parser.replace_common_unicode(\n",
    "        parser.get_text_content(\n",
    "            parser.make_soup(\n",
    "                parser.get_doc(p)\n",
    "            ), exclude=[\"title\", \"script\"]\n",
    "        )\n",
    "    )\n",
    "    text_only.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.nlp.filing_nlp import SpacyFilingTextSearch\n",
    "search = SpacyFilingTextSearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "text_only_docs = []\n",
    "for t in text_only[1:3]:\n",
    "    text_only_docs.append(search.nlp(t))\n",
    "logging.disable(logging.NOTSET)\n",
    "#TODO: determine with a profiler where i messed up and slowed this down by a magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "doc_info = defaultdict(list)\n",
    "for fp, doc in zip(file_paths, text_only_docs):\n",
    "    has_secu, has_contract = False, False\n",
    "    for sent in doc.sents:\n",
    "        for ent in sent.ents:\n",
    "            if ent.label_ == \"DATE\":\n",
    "                has_secu = True\n",
    "            if ent.label_ == \"CONTRACT\":\n",
    "                has_contract = True\n",
    "        if has_secu and has_contract:\n",
    "            doc_info[fp].append(sent)\n",
    "        has_secu, has_contract = False, False\n",
    "print(doc_info)\n",
    "# for idx, doc in enumerate(text_only_docs):\n",
    "#     doc_info[idx] = {\"ents\": defaultdict(list)}\n",
    "#     for ent in doc.ents:\n",
    "#         doc_info[idx][\"ents\"][ent.label_].append(ent)\n",
    "# for _, i in doc_info.items():\n",
    "#     for x in [\"CONTRACT\", \"SECU\"]:\n",
    "#         print(set([p.text for p in i[\"ents\"][x]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 / 42\n",
      "_comprise_of\n",
      "_issue_in\n",
      "_date_as_of\n",
      "_issue_on\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nHow to avoid creating sdps which contain more than a single fact, eg contain two other entities which have a relation two each other -> overlapping facts \\ncant filter solely based on APPOS/CONJ need to also account for the case where one ent is the token with dep_ appos or dep_ conj\\nif i filter by verb count (exlcuding aux verbs) i miss out on ?\\nif i filter by sdps only contained within ent1 and ent2, i miss out on?\\nif i filter by count or split by prep phrases? if i split i get faulty results. if i count and discard i miss out on ?\\n\\nhow can i ensure more sensible sentences are generated through sdps, \\nshould i require/add if not existant the subj (dsubj, nsubj)?\\nshould i group verbs with preps next to it before sort?\\nshould i split with prepphrase?\\nshould i filter on entity permuations within an sdp?\\n\\n\\ntake left and right groups of verbal group\\nextract valid patterns based on group patterns. example of a valid one: noun-verbal-noun\\nalso collect invalid samples and check if this approach is correct, annotate examples\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass, field, fields\n",
    "from typing import Any, Hashable\n",
    "from spacy import displacy\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from main.nlp.filing_nlp import SpacyFilingTextSearch\n",
    "search = SpacyFilingTextSearch()\n",
    "# search.nlp.tokenizer.add_special_case(\"(1)\", [{\"ORTH\": \"(1)\"}])\n",
    "# search.nlp.tokenizer.add_special_case(\"(2)\", [{\"ORTH\": \"(2)\"}])\n",
    "# logging.disable(logging.WARNING)\n",
    "sents = [\n",
    "# \"Pursuant to the purchase and sale agreement, which the Company and Nordic Inc. entered into on October 13, 2011, we sold various securities and performed a dilutive action.\",\n",
    "# \"InVivo Inc. and Nordic Inc. entered into a sales and purchase agreement on October 13, 2011.\",\n",
    "# \"Pursuant to the agreement, effective as of July 1, 2011, we sold various things.\",\n",
    "# \"As part of the offering, we entered into an amendment to the original Series C Warrant Agreement on July 18, 2011.\",\n",
    "# \"Pursuant to the agreement, which had an effective date of February 1, 2011, the Company sold to Nordic 2 for $4.0 million.\",\n",
    "# \"Subsequently, effective July 18, 2011, the Company entered into an Amendment, Settlement and Release Agreement with Solar Industries, Inc..\",\n",
    "# \"We entered into a Purchase and Sale Agreement between Lucas Energy, Inc. and HilCorp Energy I, on April 1, 2010 \",\n",
    "# \"Stock Purchase Agreement between Lucas Energy, Inc. and The Delphic Oil Co., LLC, dated December 20, 2006 \",\n",
    "# 'The selling shareholders named in this prospectus may use this prospectus to offer and resell from time to time up to 4,600,000 shares of our common stock, par value $0.0001 per share, which are comprised of (i) 140,000 shares (the “Shares”) of our common stock issued in a private placement on January 3, 2023 (the “Private Placement”), pursuant to that certain Securities Purchase Agreement by and among us and an investor, dated as of December 29, 2022 (the “Securities Purchase Agreement”), (ii) 1,860,000 shares (the “Pre-funded Warrant Shares”) of our common stock issuable upon the exercise of the pre-funded warrants (the “Pre-funded Warrants”) issued in the Private Placement pursuant to the Securities Purchase Agreement, (iii) 2,500,000 shares (the “Common Stock Warrant Shares” and together with the Pre-funded Warrant Shares, the “Warrant Shares” and collectively with the Shares, the “Registrable Securities”) of our common stock issuable upon the exercise of the warrants (the “Common Stock Warrants” and together with the Pre-funded Warrants, the “Warrants”) issued in the Private Placement pursuant to the Securities Purchase Agreement and (iv) 100,000 shares (the “Placement Agent Warrant Shares”) of our common stock issuable upon the exercise of the placement agent warrants (the “Placement Agent Warrants”) issued in connection with the Private Placement.'\n",
    "'The selling shareholders named in this prospectus may use this prospectus to offer and resell from time to time up to 4,600,000 shares of our common stock, par value $0.0001 per share, which are comprised of (i) 140,000 shares (the “Shares”) of our common stock issued in a private placement on January 3, 2023 (the “Private Placement”), pursuant to that certain Securities Purchase Agreement by and among us and an investor, dated as of December 29, 2022 (the “Securities Purchase Agreement”), (ii) 1,860,000 shares (the “Pre-funded Warrant Shares”) of our common stock issuable upon the exercise of the pre-funded warrants (the “Pre-funded Warrants”) issued in the Private Placement pursuant to the Securities Purchase Agreement, (iii) 2,500,000 shares (the “Common Stock Warrant Shares” and together with the Pre-funded Warrant Shares, the “Warrant Shares” and collectively with the Shares, the “Registrable Securities”) of our common stock issuable upon the exercise of the warrants (the “Common Stock Warrants” and together with the Pre-funded Warrants, the “Warrants”) issued in the Private Placement pursuant to the Securities Purchase Agreement and (iv) 100,000 shares (the “Placement Agent Warrant Shares”) of our common stock issuable upon the exercise of the placement agent warrants (the “Placement Agent Warrants”) issued in connection with the Private Placement.',\n",
    "'The selling shareholders named in this prospectus may use this prospectus to offer and resell from time to time up to 4,600,000 shares of our common stock, par value $0.0001 per share, which are comprised of (i) 140,000 shares of our common stock issued in a private placement on January 3, 2023, pursuant to that certain Securities Purchase Agreement by and among us and an investor, dated as of December 29, 2022, (ii) 1,860,000 shares of our common stock issuable upon the exercise of the pre-funded warrants issued in the Private Placement pursuant to the Securities Purchase Agreement, (iii) 2,500,000 shares of our common stock issuable upon the exercise of the warrants issued in the Private Placement pursuant to the Securities Purchase Agreement and (iv) 100,000 shares of our common stock issuable upon the exercise of the placement agent warrants issued in connection with the Private Placement.',\n",
    "# '140,000 shares (the “Shares”) of our common stock issued in a private placement on January 3, 2023 (the “Private Placement”), pursuant to that certain Securities Purchase Agreement by and among us and an investor, dated as of December 29, 2022 (the “Securities Purchase Agreement”)',\n",
    "# '10 class B preferred shares are comprised of 140,000 shares of our common stock issued in a private placement on January 3, 2023, pursuant to that certain Securities Purchase Agreement by and among us and an investor, dated as of December 29, 2022, and 500 class C restricted shares',\n",
    "# '10 class B preferred shares are comprised of 140,000 shares of our common stock issued in a private placement on January 3, 2023, 3000 class B senior notes, and 500 class C restricted shares',\n",
    "# '10 class B preferred shares are comprised of 1,860,000 shares (the “Pre-funded Warrant Shares”) of our common stock issuable upon the exercise of the pre-funded warrants (the “Pre-funded Warrants”) issued in the Private Placement pursuant to the Securities Purchase Agreement',\n",
    "# '10 class B preferred shares are comprised of 2,500,000 shares of our common stock issuable upon the exercise of the warrants issued in the Private Placement pursuant to the Securities Purchase Agreement',\n",
    "# '10 class B preferred shares are comprised of 100,000 shares (the “Placement Agent Warrant Shares”) of our common stock issuable upon the exercise of the placement agent warrants (the “Placement Agent Warrants”) issued in connection with the Private Placement.'\n",
    "]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Chain:\n",
    "    items: list = field(default_factory=list)\n",
    "    _unique: set = field(default_factory=set)\n",
    "    root: Any = field(default=None)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.items != []:\n",
    "            for item in self.items:\n",
    "                self._unique.add(item)\n",
    "\n",
    "    def add(self, item):\n",
    "        if item not in self._unique:\n",
    "            self.items.append(item)\n",
    "            self._unique.add(item)\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(tuple(self._unique))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.items)\n",
    "\n",
    "\n",
    "def get_conditional_chains(token, validation_function) -> Chain:\n",
    "    chains = set()\n",
    "    visited = set()\n",
    "    queue = [[token]]\n",
    "    while queue:\n",
    "        path = queue.pop(0)\n",
    "        node = path[-1]\n",
    "        if node in visited:\n",
    "            continue\n",
    "        visited.add(node)\n",
    "        for child in node.children:\n",
    "            if validation_function(child, path) is True:\n",
    "                new_path = list(path)\n",
    "                new_path.append(child)\n",
    "                queue.append(new_path)\n",
    "            else:\n",
    "                if len(path) > 1:\n",
    "                    chain = Chain(path, root=token)\n",
    "                    if chain:\n",
    "                        chains.add(chain)\n",
    "    return tuple(chains)\n",
    "\n",
    "\n",
    "def is_valid_prep_child(node, path):\n",
    "    if node.dep_ in [\"pobj\", \"prep\"]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_valid_amod_chain(node, path):\n",
    "    if node.dep_ == \"amod\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_preps(token):\n",
    "    amods = list(filter(lambda x: x.dep_ == \"amod\", list(token.children)))\n",
    "    chains = []\n",
    "    if amods != []:\n",
    "        for amod in amods:\n",
    "            chains.append(get_conditional_chains(amod, is_valid_prep_child))\n",
    "    chains.append(get_conditional_chains(token, is_valid_prep_child))\n",
    "    return chains\n",
    "\n",
    "def get_dobjs(token):\n",
    "    dobjs = []\n",
    "    for child in token.children:\n",
    "        if child.dep_ == \"dobj\":\n",
    "            dobj = [child]\n",
    "            preps = get_preps(child)\n",
    "            for prep in preps:\n",
    "                dobjs.append(prep)\n",
    "            if not preps:\n",
    "                dobjs.append(dobj)\n",
    "    return dobjs\n",
    "            \n",
    "def get_parent_verb(token):\n",
    "    next_token = token.head\n",
    "    t = None\n",
    "    while next_token:\n",
    "        # print(next_token)\n",
    "        t = next_token\n",
    "        if t.pos_ == \"VERB\":\n",
    "            return t\n",
    "        next_token = t.head\n",
    "        if t.head == t:\n",
    "            # print(t, t.pos_, t.dep_, list(t.ancestors))\n",
    "            break\n",
    "    return None\n",
    "\n",
    "def get_child_verbs(token):\n",
    "    verbs = []\n",
    "    visited = set()\n",
    "    queue = [[token]]\n",
    "    while queue:\n",
    "        path = queue.pop(0)\n",
    "        node = path[-1]\n",
    "        if node in visited:\n",
    "            continue\n",
    "        visited.add(node)\n",
    "        for child in node.children:\n",
    "            if child.pos_ != \"VERB\":\n",
    "                new_path = list(path)\n",
    "                new_path.append(child)\n",
    "                queue.append(new_path)\n",
    "            else:\n",
    "                verbs.append(child)\n",
    "    return verbs\n",
    "\n",
    "def get_subj(token):\n",
    "    subjs = []\n",
    "    visited = set()\n",
    "    queue = [[token]]\n",
    "    while queue:\n",
    "        path = queue.pop(0)\n",
    "        node = path[-1]\n",
    "        if node in visited:\n",
    "            continue\n",
    "        visited.add(node)\n",
    "        for child in node.children:\n",
    "            if child.dep_ not in [\"nsubj\"]:\n",
    "                new_path = list(path)\n",
    "                new_path.append(child)\n",
    "                queue.append(new_path)\n",
    "            else:\n",
    "                subjs.append(child)\n",
    "    return subjs\n",
    "\n",
    "\n",
    "\n",
    "def get_verb_targets(token):\n",
    "    if token.pos_ != \"VERB\":\n",
    "        raise TypeError(\"only tokens with pos 'VERB' are accepted\")\n",
    "    if token.head:\n",
    "        if token.dep_ in [\"acl\", \"nsubj\"]:\n",
    "            return token.head\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_sdp_path(doc, head, tail, lca_matrix):\n",
    "    lca = lca_matrix[head, tail]\n",
    "    current_node = doc[head]\n",
    "    head_path = [current_node]\n",
    "    if lca != -1: \n",
    "        if lca != head: \n",
    "            while current_node.head.i != lca:\n",
    "                current_node = current_node.head\n",
    "                head_path.append(current_node)\n",
    "            head_path.append(current_node.head)\n",
    "    current_node = doc[tail]\n",
    "    tail_path = [current_node]\n",
    "    if lca != -1: \n",
    "        if lca != tail: \n",
    "            while current_node.head.i != lca:\n",
    "                current_node = current_node.head\n",
    "                tail_path.append(current_node)\n",
    "            tail_path.append(current_node.head)\n",
    "    return head_path + tail_path[::-1][1:]\n",
    "\n",
    "from main.nlp.filing_nlp_utils import extend_token_ent_to_span\n",
    "from spacy.tokens import Span, Token\n",
    "seen_sents = set()\n",
    "\n",
    "def default_ent_filter(ents):\n",
    "    f = list(filter(lambda x: x.label_ not in [\"MONEY\", \"ORDINAL\", \"CARDINAL\"], ents))\n",
    "    for ent1 in f:\n",
    "        for ent2 in f:\n",
    "            if ent1 == ent2:\n",
    "                continue\n",
    "            else:\n",
    "                yield ent1, ent2\n",
    "                # if ent1.start < ent2.start:\n",
    "                #     yield ent1, ent2\n",
    "                # else:\n",
    "                #     yield ent2, ent1\n",
    "\n",
    "def distance_ent_filter(ents, max_distance: int=150):\n",
    "    valid_ent_pairs = set()\n",
    "    ent_to_idx_map = {x: ent for ent in ents for x in range(ent.start, ent.end + 1, 1)}\n",
    "    for ent in ents:\n",
    "        max_left = ent.start - max_distance\n",
    "        max_left = max_left if max_left > 0 else 0\n",
    "        left = max_left if max_left > ent.sent.start else ent.sent.start\n",
    "        max_right = ent.end + max_distance\n",
    "        right = max_right if max_right < ent.sent.end else ent.sent.end\n",
    "        for i in range(left, ent.start):\n",
    "            try:\n",
    "                l_ent = ent_to_idx_map[i]\n",
    "            except KeyError:\n",
    "                continue\n",
    "            if l_ent:\n",
    "                if (l_ent, ent) not in valid_ent_pairs:\n",
    "                    valid_ent_pairs.add((l_ent, ent))\n",
    "                    yield l_ent, ent\n",
    "        for ri in range(ent.end, right+1):\n",
    "            try:\n",
    "                r_ent = ent_to_idx_map[ri]\n",
    "            except KeyError:\n",
    "                continue\n",
    "            if r_ent:\n",
    "                if (ent, r_ent) not in valid_ent_pairs:\n",
    "                    valid_ent_pairs.add((ent, r_ent))\n",
    "                    yield ent, r_ent\n",
    "\n",
    "def is_valid_ent_type_combination(ent1, ent2):\n",
    "    for left, right in [\n",
    "        (\"DATE\", \"DATE\"),\n",
    "        (\"CONTRACT\", \"CONTRACT\"),\n",
    "    ]:  \n",
    "        if ent1.label_ == left and ent2.label_ == right:\n",
    "            return False\n",
    "        else:\n",
    "            labels = [ent1.label_, ent2.label_]\n",
    "            blocked_labels = [\"ODRINAL\", \"CARDINAL\", \"MONEY\", \"SECU\"] \n",
    "            if (ent1.label_ in blocked_labels) or (ent2.label_ in blocked_labels):\n",
    "                return False\n",
    "    return doesnt_contain_base_alias(ent1, ent2)\n",
    "\n",
    "\n",
    "def doesnt_contain_base_alias(ent1, ent2):\n",
    "    for ent in [ent1, ent2]:\n",
    "        aliases = set([i._.containing_alias_span for i in ent])\n",
    "        if aliases != set([None]):\n",
    "            for alias in aliases:\n",
    "                if alias in doc._.alias_cache._base_alias_set:\n",
    "                    return False\n",
    "    return True\n",
    "                \n",
    "def filter_unwanted_tokens_from_sdp(sdp_tuple: tuple[Span, Span, list[Token]]):\n",
    "    new_sdp = []\n",
    "    left, right, sdp = sdp_tuple\n",
    "    parents_to_remove = set()\n",
    "    for t in sdp:\n",
    "        if t.dep_ in [\"conj\", \"appos\"]:\n",
    "            if t.pos_ == t.head.pos_:\n",
    "                if t.head not in left and t.head not in right:\n",
    "                    parents_to_remove.add(t.head.i)\n",
    "    for t in sdp:\n",
    "        if t.i in parents_to_remove:\n",
    "            continue\n",
    "        # if (t not in left) and (t not in right):\n",
    "        #     if t.dep_ in [\"conj\", \"appos\"]:\n",
    "        #         continue\n",
    "        #     else:\n",
    "        #         pass\n",
    "        new_sdp.append(t)\n",
    "    return (sdp_tuple[0], sdp_tuple[1], new_sdp)\n",
    "\n",
    "def is_valid_sdp(sdp_tuple: tuple[Span, Span, list[Token]]):\n",
    "    left, right, sdp = sdp_tuple\n",
    "    # verbs = list(filter(lambda x: x.pos_ == \"VERB\", sdp))\n",
    "    if (\n",
    "        (len(sdp) > 2)\n",
    "    ):\n",
    "        return True\n",
    "    # print(f\"not a valid sdp, (left, right, sdp): {left, right, sdp}\")\n",
    "    return False\n",
    "\n",
    "def group_elements_of_sdp(sdp):\n",
    "    # TODO: implement patterns\n",
    "    # assuming sdp is sorted by token idx\n",
    "    groups = []\n",
    "    used_idx = set()\n",
    "    for idx, token in enumerate(sdp):\n",
    "        if idx in used_idx:\n",
    "            continue\n",
    "        if token.pos_ == \"VERB\":\n",
    "            group = [token]\n",
    "            used_idx.add(idx)\n",
    "            for i in range(idx+1, len(sdp)):\n",
    "                if i in used_idx:\n",
    "                    break\n",
    "                if sdp[i].dep_ == \"prep\":\n",
    "                    group.append(sdp[i])\n",
    "                    used_idx.add(i)\n",
    "                else:\n",
    "                    break\n",
    "            groups.append({\"name\": \"verbal\", \"group\": group})\n",
    "\n",
    "    for idx, token in enumerate(sdp):\n",
    "        if idx in used_idx:\n",
    "            continue\n",
    "        if token.pos_ == \"ADJ\" and token.dep_ == \"amod\":\n",
    "            group = [token]\n",
    "            used_idx.add(idx)\n",
    "            for i in range(idx+1, len(sdp)):\n",
    "                if i in used_idx:\n",
    "                    break\n",
    "                if sdp[i].dep_ == \"prep\":\n",
    "                    group.append(sdp[i])\n",
    "                    used_idx.add(i)\n",
    "                else:\n",
    "                    break\n",
    "            groups.append({\"name\": \"amod\", \"group\": group})\n",
    "\n",
    "    for idx, token in enumerate(sdp):\n",
    "        if idx in used_idx:\n",
    "            continue\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            group = [token]\n",
    "            used_idx.add(idx)\n",
    "            for i in range(idx+1, len(sdp)):\n",
    "                if i in used_idx:\n",
    "                    break\n",
    "                if sdp[i].pos_ in [\"NOUN\", \"PROPN\"] or sdp[i].dep_ == \"prep\":\n",
    "                    group.append(sdp[i])\n",
    "                    used_idx.add(i)\n",
    "                else:\n",
    "                    break\n",
    "            for i in range(idx-1, -1, -1):\n",
    "                if i in used_idx:\n",
    "                    break\n",
    "                if (sdp[i].pos_ in [\"NOUN\", \"PROPN\"]) or (sdp[i].dep_ in [\"prep\", \"nummod\", \"nmod\", \"compound\", \"det\"]):\n",
    "                    group.insert(0, sdp[i])\n",
    "                    used_idx.add(i)\n",
    "                else:\n",
    "                    break\n",
    "            groups.append({\"name\": \"noun\", \"group\": group})\n",
    "        \n",
    "    \n",
    "    for idx, token in enumerate(sdp):\n",
    "        if idx in used_idx:\n",
    "            continue\n",
    "        if token.pos_ == \"ADV\":\n",
    "            group = [token]\n",
    "            used_idx.add(idx)\n",
    "            for i in range(idx+1, len(sdp)):\n",
    "                if i in used_idx:\n",
    "                    break\n",
    "                if sdp[i].dep_ == \"prep\":\n",
    "                    group.append(sdp[i])\n",
    "                    used_idx.add(i)\n",
    "                else:\n",
    "                    break\n",
    "            groups.append({\"name\": \"adverbal\", \"group\": group})\n",
    "\n",
    "    for idx, token in enumerate(sdp):\n",
    "        if idx in used_idx: continue\n",
    "        group = [token]\n",
    "        for i in range(idx+1, len(sdp)):\n",
    "            if i in used_idx:\n",
    "                break\n",
    "            else:\n",
    "                group.append(sdp[i])\n",
    "                used_idx.add(i)\n",
    "\n",
    "        for i in range(idx-1, -1, -1):\n",
    "            if i in used_idx:\n",
    "                break\n",
    "            else:\n",
    "                group.insert(0, sdp[i])\n",
    "                used_idx.add(i)\n",
    "        groups.append({\"name\": \"unknown\", \"group\": group})\n",
    "        \n",
    "    return sorted(groups, key=lambda x: x[\"group\"][0].i)\n",
    "\n",
    "            \n",
    "\n",
    "def sdp_counts(sdp_tuple):\n",
    "    left, right, sdp = sdp_tuple\n",
    "    verb_count, noun_count, prep_count, obj_count, pobj_count, dobj_count = 0, 0, 0, 0, 0, 0\n",
    "    for x in sdp:\n",
    "        if x.pos_ == \"VERB\":\n",
    "            verb_count += 1\n",
    "        if x.pos_ == \"NOUN\":\n",
    "            noun_count += 1\n",
    "        if x.dep_ in [\"pobj\", \"dobj\"]:\n",
    "            obj_count += 1\n",
    "        if x.dep_ == \"pobj\":\n",
    "            pobj_count += 1\n",
    "        if x.dep_ == \"dobj\":\n",
    "            dobj_count += 1\n",
    "        if x.dep_ == \"prep\":\n",
    "            prep_count += 1\n",
    "    return (verb_count, noun_count, prep_count, obj_count, pobj_count, dobj_count)\n",
    "\n",
    "def sdp_between_ents(doc, ent_filter) -> dict[tuple[Span, Span], tuple[Token]]:\n",
    "    shortest_dependency_paths = []\n",
    "    lca = doc.get_lca_matrix()\n",
    "    seen_combinations = set()\n",
    "    for sent in doc.sents:\n",
    "        for ent1, ent2 in ent_filter(sent.ents):\n",
    "            left, right = None, None\n",
    "            if ent1.root.i < ent2.root.i:\n",
    "                left, right = ent1, ent2\n",
    "            else:\n",
    "                left, right = ent2, ent1\n",
    "            if is_valid_ent_type_combination(left, right):\n",
    "                if ((left is not None) or (right is not None)) and (left, right) not in seen_combinations:\n",
    "                    seen_combinations.add((left, right))\n",
    "                    sdp = get_sdp_path(doc, left.root.i, right.root.i, lca)\n",
    "                    shortest_dependency_paths.append((left, right, sdp))\n",
    "    return shortest_dependency_paths\n",
    "\n",
    "def get_sdps(doc, ent_filter=default_ent_filter, sdp_token_filter=filter_unwanted_tokens_from_sdp, sdp_validation_function=is_valid_sdp):\n",
    "    valid_sdps = []\n",
    "    discarded = []\n",
    "    sdp_tuples = sdp_between_ents(doc, ent_filter=ent_filter)\n",
    "    for sdp_tuple in sdp_tuples:\n",
    "        filtered_tuple = sdp_token_filter(sdp_tuple)\n",
    "        if sdp_validation_function(filtered_tuple) is True:\n",
    "            valid_sdps.append(filtered_tuple)\n",
    "        else:\n",
    "            discarded.append((sdp_tuple, filtered_tuple))\n",
    "    return valid_sdps, discarded\n",
    "\n",
    "def get_sdps_noun_verbal(docs):\n",
    "    sdp_groups = []\n",
    "    for doc in docs:\n",
    "        doc = search.nlp(text)\n",
    "        sdps, discarded = get_sdps(doc)\n",
    "        for i, x in enumerate(sdps):\n",
    "            left, right, sdp = x\n",
    "            sorted_sdp = sorted(sdp, key=lambda x: x.i)\n",
    "            sdp_grouped = group_elements_of_sdp(sorted_sdp)\n",
    "            sdp_groups.append((left, right, sdp_grouped, x[2]))\n",
    "    noun_verbal_unknown_sdps = list(filter(lambda x: (all([i[\"name\"] in [\"noun\", \"verbal\", \"unknown\"] for i in x[2]]) and (len(x[2]) <= 3)) , sdp_groups))\n",
    "    return noun_verbal_unknown_sdps\n",
    "\n",
    "def create_sdp_noun_verbal_dict(left, right, sdp_grouped, sdp):\n",
    "    nvu_info = {\"groups\": [], \"relation\": None}\n",
    "    for sdp_group in sdp_grouped:\n",
    "        values = sdp_group[\"group\"]\n",
    "        key = sdp_group[\"name\"]\n",
    "        group = {\n",
    "            \"type\": key,\n",
    "            \"tokens\": [t.i for t in values]\n",
    "        }\n",
    "        nvu_info[\"groups\"].append(group)\n",
    "        if key == \"verbal\":\n",
    "            if nvu_info[\"relation\"] is not None:\n",
    "                raise ValueError(\"only one verbal relation allowed, ensure only nvu's[len==3] are passed in.\")\n",
    "            relation = \"\"\n",
    "            for t in values:\n",
    "                if t.pos_ == \"VERB\":\n",
    "                    relation = \"_\".join([relation, t.lemma_])\n",
    "                else:\n",
    "                    relation = \"_\".join([relation, t.lower_])\n",
    "            nvu_info[\"relation\"] = relation\n",
    "    nvu_info[\"sdp\"] = [{\"token\": t, \"lemma\": t.lemma_, \"pos\": t.pos_, \"dep\": t.dep_, \"i\": t.i} for t in sdp]\n",
    "    for name, ent in zip([\"left\", \"right\"], [left, right]):\n",
    "        nvu_info[name] = {\n",
    "            \"tokens\": [{\"token\": t, \"lemma\": t.lemma_, \"pos\": t.pos_, \"dep\": t.dep_, \"i\": t.i} for t in ent],\n",
    "            \"text\": ent.text\n",
    "        }\n",
    "    nvu_info[\"sent\"] = left.sent\n",
    "    return nvu_info\n",
    "\n",
    "    \n",
    "def get_subjs_for_sdp(sdp_tuple):\n",
    "    left, right, sdp = sdp_tuple\n",
    "    has_subj = False\n",
    "    subjs = []\n",
    "    for t in sdp:\n",
    "        if t.dep_ in [\"nsubj\", \"nsubjpass\", \"dobj\", \"pobj\"]:\n",
    "            has_subj = True\n",
    "            subjs.append(t)\n",
    "    if not has_subj:\n",
    "        subjs = get_subj(left.sent.root)         \n",
    "    return subjs\n",
    "\n",
    "\n",
    "                # get subjs by appos, conj if we end chain in nsubj else discard chain\n",
    "                # dfs instead of bfs\n",
    "                # already implemented in utils ...\n",
    "                            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "docs = []\n",
    "for text in sents:\n",
    "    doc = search.nlp(text)\n",
    "    docs.append(doc)\n",
    "\n",
    "\n",
    "nvu_sdps = get_sdps_noun_verbal(docs)\n",
    "relations = set()\n",
    "for left, right, sdp_grouped, sdp in nvu_sdps:\n",
    "    info = create_sdp_noun_verbal_dict(left, right, sdp_grouped, sdp)\n",
    "    if info[\"relation\"] is not None:\n",
    "        relations.add(info[\"relation\"])\n",
    "    # print(info)\n",
    "print(len(relations),\"/\",len(nvu_sdps))\n",
    "for rel in relations:\n",
    "    print(rel)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    " \n",
    "    # short_groups = list(filter(lambda x: len(x[2]) < 7, sdp_groups))\n",
    "    # for sdp in nvns:\n",
    "    #     print(\">------------\")\n",
    "    #     print(sdp[0], \" | \", sdp[1])\n",
    "    #     print([f\"{str(i['group']): ^30}\" for i in sdp[2]])\n",
    "    #     print([f\"{str(i['name']): ^30}\" for i in sdp[2]])\n",
    "\n",
    "\n",
    "        # displacy.render(raw, manual=True, options={\"compact\": True, \"word_spacing\": 16, \"arrow_spacing\": 10})\n",
    "    # print(ent_connections)\n",
    "    # print(sdps)\n",
    "\n",
    "\n",
    "    # print(search.get_prep_phrases(doc))\n",
    "        # print(search.get_verbal_phrases(doc))\n",
    "\n",
    "        \n",
    "#     displacy.render(doc, style=\"ent\", options={\n",
    "#         \"ents\": [\"SECU\", \"SECUQUANTITY\", \"CONTRACT\", \"DATE\", \"ORG\", \"PLACEMENT\"],\n",
    "#         \"colors\": {\"SECU\": \"cyan\", \"SECUQUANTITY\": \"red\", \"CONTRACT\": \"green\", \"DATE\": \"yellow\", \"ORG\": \"blue\", \"PLACEMENT\":\"purple\"}\n",
    "#         }\n",
    "#     )\n",
    "#     displacy.render(doc,\n",
    "#             style=\"dep\",\n",
    "#             options={\"fine_grained\": True, \"compact\": True, \"word_spacing\": 16, \"arrow_spacing\": 12}\n",
    "#         )\n",
    "# print(\"______________________\")\n",
    "logging.disable(logging.NOTSET)\n",
    "\n",
    "'''\n",
    "How to avoid creating sdps which contain more than a single fact, eg contain two other entities which have a relation two each other -> overlapping facts \n",
    "cant filter solely based on APPOS/CONJ need to also account for the case where one ent is the token with dep_ appos or dep_ conj\n",
    "if i filter by verb count (exlcuding aux verbs) i miss out on ?\n",
    "if i filter by sdps only contained within ent1 and ent2, i miss out on?\n",
    "if i filter by count or split by prep phrases? if i split i get faulty results. if i count and discard i miss out on ?\n",
    "\n",
    "how can i ensure more sensible sentences are generated through sdps, \n",
    "should i require/add if not existant the subj (dsubj, nsubj)?\n",
    "should i group verbs with preps next to it before sort?\n",
    "should i split with prepphrase?\n",
    "should i filter on entity permuations within an sdp?\n",
    "\n",
    "\n",
    "take left and right groups of verbal group\n",
    "extract valid patterns based on group patterns. example of a valid one: noun-verbal-noun\n",
    "also collect invalid samples and check if this approach is correct, annotate examples\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E004] Can't set up pipeline component: a factory for 'bert_re' already exists. Existing factory: <function create_bert_re at 0x0000026C32B5A050>. New factory: <function create_bert_re at 0x0000026C33DE5C60>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 30\u001b[0m\n\u001b[0;32m     26\u001b[0m                         doc\u001b[39m.\u001b[39ments \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(doc\u001b[39m.\u001b[39ments) \u001b[39m+\u001b[39m [span]\n\u001b[0;32m     27\u001b[0m         \u001b[39mreturn\u001b[39;00m doc\n\u001b[0;32m     29\u001b[0m \u001b[39m@Language\u001b[39;49m\u001b[39m.\u001b[39;49mfactory(\u001b[39m\"\u001b[39;49m\u001b[39mbert_re\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m---> 30\u001b[0m \u001b[39mdef\u001b[39;49;00m \u001b[39mcreate_bert_re\u001b[39;49m(name, nlp):\n\u001b[0;32m     31\u001b[0m     \u001b[39mreturn\u001b[39;49;00m BertRelationExtractor(\u001b[39m\"\u001b[39;49m\u001b[39mbert-base-uncased\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnlp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfiling_nlp\u001b[39;00m \u001b[39mimport\u001b[39;00m SpacyFilingTextSearch\n",
      "File \u001b[1;32mc:\\Users\\Olivi\\.virtualenvs\\sec_scraping_testing-er4NC4pi\\lib\\site-packages\\spacy\\language.py:489\u001b[0m, in \u001b[0;36mLanguage.factory.<locals>.add_factory\u001b[1;34m(factory_func)\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m util\u001b[39m.\u001b[39mis_same_func(factory_func, existing_func):\n\u001b[0;32m    486\u001b[0m         err \u001b[39m=\u001b[39m Errors\u001b[39m.\u001b[39mE004\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    487\u001b[0m             name\u001b[39m=\u001b[39mname, func\u001b[39m=\u001b[39mexisting_func, new_func\u001b[39m=\u001b[39mfactory_func\n\u001b[0;32m    488\u001b[0m         )\n\u001b[1;32m--> 489\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err)\n\u001b[0;32m    491\u001b[0m arg_names \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mget_arg_names(factory_func)\n\u001b[0;32m    492\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnlp\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m arg_names \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m arg_names:\n",
      "\u001b[1;31mValueError\u001b[0m: [E004] Can't set up pipeline component: a factory for 'bert_re' already exists. Existing factory: <function create_bert_re at 0x0000026C32B5A050>. New factory: <function create_bert_re at 0x0000026C33DE5C60>"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "for fp, sents in doc_info.items():\n",
    "    print(fp)\n",
    "    displacy.render(sents, style=\"ent\", options={\n",
    "        # \"ents\": [\"SECU\", \"SECUQUANTITY\", \"CONTRACT\", \"PLACEMENT\", \"DATE\", \"ORG\"],\n",
    "        \"ents\": [\"SECU\", \"CONTRACT\", \"DATE\"],\n",
    "        \"colors\": {\"SECU\": \"cyan\", \"SECUQUANTITY\": \"purple\", \"CONTRACT\": \"green\", \"PLACEMENT\": \"red\", \"DATE\": \"yellow\", \"ORG\": \"blue\"}\n",
    "        }\n",
    "    )\n",
    "    print(\"----------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from main.nlp.filing_nlp import SpacyFilingTextSearch\n",
    "from spacy import displacy\n",
    "from spacy import explain\n",
    "\n",
    "search = SpacyFilingTextSearch()\n",
    "\n",
    "def displacy_dep_with_search(text, print_tokens=False, show_lemmas=False, finegrained=False):\n",
    "        doc = search.nlp(text)\n",
    "        if print_tokens is True:\n",
    "            for token in doc:\n",
    "                print(token)\n",
    "                print(      explain(token.dep_),  token.dep_)\n",
    "                print(      explain(token.pos_), token.pos_)\n",
    "                print(      explain(token.tag_), token.tag_)\n",
    "        displacy.render(\n",
    "            doc,\n",
    "            style=\"dep\",\n",
    "            options={\"fine_grained\": finegrained, \"compact\": True, \"add_lemma\": show_lemmas, \"word_spacing\": 16, \"arrow_spacing\": 12}\n",
    "        )\n",
    "\n",
    "def displacy_ent_with_search(text, print_ents: bool=True, print_tokens: bool=True):\n",
    "    doc = search.nlp(text)\n",
    "    if print_ents:\n",
    "        print(\"------\")\n",
    "        print(\"ENTS:\")\n",
    "        print([(i, i.label_) for i in doc.ents])\n",
    "    if print_tokens:\n",
    "        print(\"------\")\n",
    "        print(\"TOKENS:\")\n",
    "        print([i for i in doc])\n",
    "    displacy.render(doc, style=\"ent\", options={\n",
    "        \"ents\": [\"SECU\", \"SECUQUANTITY\", \"CONTRACT\", \"PLACEMENT\", \"DATE\", \"ORG\"],\n",
    "        \"colors\": {\"SECU\": \"cyan\", \"SECUQUANTITY\": \"purple\", \"CONTRACT\": \"green\", \"PLACEMENT\": \"red\", \"DATE\": \"yellow\", \"ORG\": \"blue\"}\n",
    "        }\n",
    "    )\n",
    "text = \"On March 8, 2021, Hoth Therapeutics, Inc. (the “Company”) entered into a securities purchase agreement (the “Purchase Agreement”) with certain institutional and accredited investors (the “Investors”) pursuant to which it agreed to sell an aggregate of (i) 6,826,962 shares (the “Shares”) of common stock, par value $0.0001 per share (the “Common Stock”), (ii) warrants (the “Pre-Funded Warrants”) to purchase up to 767,975 shares (the “Pre-funded Warrant Shares”) of Common Stock and (iii) warrants (the “Common Stock Warrants” and together with the Pre-Funded Warrants, the “Warrants”) to purchase up to 7,594,937 shares (the “Warrant Shares” and together with the Shares and the Pre-Funded Warrant Shares, the “Registrable Securities”) of Common Stock at a purchase price of $1.975 per share and accompanying warrant in a private placement for aggregate gross proceeds of approximately $15 million, exclusive of placement agent commission and fees and other offering expenses (the “Offering”). The closing of the Offering is expected to occur on March 10, 2021, subject to the satisfaction of customary closing conditions.\"\n",
    "\n",
    "# displacy_ent_with_search(\n",
    "#     text\n",
    "#     # \"On February 22, 2021, we entered into the Securities Purchase Agreement (the “Securities Purchase Agreement”), pursuant to which we agreed to issue the investor named therein (the “Investor”) 8,888,890 shares (the “Shares”) of our common stock, par value $0.000001 per share, at a purchase price of $2.25 per share, and a warrant to purchase up to 6,666,668 shares of our common stock (the “Investor Warrant”) in a private placement (the “Private Placement”). The closing of the Private Placement occurred on February 24, 2021. Pursuant to the Securities Purchase Agreement we also issued 10000 shares of our common stock through the Investor Warrant.\"\n",
    "# #     # \"The selling stockholders acquired these shares from us pursuant to a (i) Securities Purchase Agreement, dated February 22, 2021 pursuant to which we issued 8,888,890 shares of common stock, par value $0.000001 per share, at a purchase price of $2.25 per share, and a warrant to purchase up to 6,666,668 shares of common stock in a private placement, and (ii) Placement Agent Agreement, dated February 22, 2021, with A.G.P./Alliance Global Partners pursuant to which we issued warrants to purchase up to an aggregate of 444,444 shares of common stock.\"\n",
    "# #     # \"Also on February 22, 2021, we entered into a placement agent agreement with the Placement Agent pursuant to which the Placement Agent served as our exclusive placement agent in connection with the Private Placement (the “Placement Agent Agreement”). Pursuant to the Placement Agent Agreement, we agreed to pay the Placement Agent a fee equal to a certain percentage of the aggregate gross proceeds from the Private Placement. In addition to the cash fee, we issued to the Placement Agent, warrants to purchase up to 5.0% of the Shares sold to the Investor in the Private Placement, or 444,444 shares of our common stock (the “Placement Agent Warrants”). The Placement Agent Warrants are immediately exercisable and expire on the five-year anniversary of the date of issuance. The Placement Agent Warrants have an exercise price of $2.8125 per share, subject to customary anti-dilution, but not price protection, adjustments\"\n",
    "# #     \"Also on February 22, 2021, we entered into a Private Placement Agreement (the “Private Placement Agent Agreement”).\"\n",
    "# )\n",
    "\n",
    "# from main.nlp.filing_nlp_coref_setter import create_coref_setter\n",
    "# search = SpacyFilingTextSearch()\n",
    "# # search.nlp.add_pipe(\"coref_setter\")\n",
    "# doc = search.nlp(text)\n",
    "# for origin in list(doc._.alias_cache._origin_base_alias_map.keys()):\n",
    "#     doc._.alias_cache._pretty_print_tree_from_origin(origin, doc)\n",
    "\n",
    "\n",
    "\n",
    "# displacy_dep_with_search(\n",
    "#     # \"On February 22, 2021, we entered into the Securities Purchase Agreement (the “Securities Purchase Agreement”), pursuant to which we agreed to issue the investor named therein (the “Investor”) 8,888,890 shares (the “Shares”) of our common stock, par value $0.000001 per share, at a purchase price of $2.25 per share, and a warrant to purchase up to 6,666,668 shares of our common stock (the “Investor Warrant”) in a private placement (the “Private Placement”). The closing of the Private Placement occurred on February 24, 2021. Pursuant to the Securities Purchase Agreement we also issued 10000 shares of our common stock through the Investor Warrant.\"\n",
    "#     # \"On March 8, 2021, Hoth Therapeutics, Inc. (the “Company”) entered into a securities purchase agreement (the “Purchase Agreement”) with certain institutional and accredited investors (the “Investors”) pursuant to which it agreed to sell an aggregate of (i) 6,826,962 shares (the “Shares”) of common stock, par value $0.0001 per share (the “Common Stock”), (ii) warrants (the “Pre-Funded Warrants”) to purchase up to 767,975 shares (the “Pre-funded Warrant Shares”) of Common Stock and (iii) warrants (the “Common Stock Warrants” and together with the Pre-Funded Warrants, the “Warrants”) to purchase up to 7,594,937 shares (the “Warrant Shares” and together with the Shares and the Pre-Funded Warrant Shares, the “Registrable Securities”) of Common Stock at a purchase price of $1.975 per share and accompanying warrant in a private placement for aggregate gross proceeds of approximately $15 million, exclusive of placement agent commission and fees and other offering expenses (the “Offering”). The closing of the Offering is expected to occur on March 10, 2021, subject to the satisfaction of customary closing conditions.\"\n",
    "#     text\n",
    "# #     \"Also on February 22, 2021, we entered into a Private Placement Agreement (the “Private Placement Agent Agreement”).\"\n",
    "\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy_dep_with_search('The Company sold an aggregate of 2,510,506 units pursuant to a Securities Purchase Agreement (the \" Purchase Agreement \") to certain institutional investors (the \" Investors \").')\n",
    "displacy_dep_with_search('We sold 10 units, each consisting of (a) one share of our common stock; (b) one Series B Warrant to purchase one share of our common stock at an exercise price of $2.86 per share (the \"Series B Warrants\"); and (c) one Series C Warrant to purchase one share of our common stock at an exercise price of $2.62 per share.', finegrained=True)\n",
    "\n",
    "text = 'We sold 10 units, each consisting of (a) one share of our common stock; (b) one Series B Warrant to purchase one share of our common stock at an exercise price of $2.86 per share; and (c) one Series C Warrant to purchase one share of our common stock at an exercise price of $2.62 per share.'\n",
    "doc = search.nlp(text)\n",
    "#Enumeration\n",
    "items = []\n",
    "start, end = None, None\n",
    "for token in doc:\n",
    "    if token.tag_ == \"LS\":\n",
    "        start = token.i\n",
    "        end = None\n",
    "    if token.lower_ in [\";\", \".\"] or (token.tag_ == \"LS\" and token.i != start):\n",
    "        end = token.i\n",
    "    if start and end:\n",
    "        items.append(doc[start:end])\n",
    "        start, end = None, None\n",
    "for i in items:\n",
    "    print(i)\n",
    "\n",
    "#Scope\n",
    "scope = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"CONTRACT\":\n",
    "        root_verb = search.secu_attr_getter.get_root_verb(ent.root)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main.nlp.filing_nlp_alias_setter import AliasMatcher, AliasSetter\n",
    "from main.nlp.filing_nlp import SECUMatcher, create_secu_matcher\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.add_pipe(\"secu_matcher\")\n",
    "alias_matcher = AliasMatcher()\n",
    "alias_setter = AliasSetter(nlp.vocab)\n",
    "test_text =  \"On March 8, 2021, Hoth Therapeutics, Inc. (the “Company”) entered into a securities purchase agreement (the “Purchase Agreement”) with certain institutional and accredited investors (the “Investors”) pursuant to which it agreed to sell an aggregate of (i) 6,826,962 shares (the “Shares”) of common stock, par value $0.0001 per share (the “Common Stock”), (ii) warrants (the “Pre-Funded Warrants”) to purchase up to 767,975 shares (the “Pre-funded Warrant Shares”) of Common Stock and (iii) warrants (the “Common Stock Warrants” and together with the Pre-Funded Warrants, the “Warrants”) to purchase up to 7,594,937 shares (the “Warrant Shares” and together with the Shares and the Pre-Funded Warrant Shares, the “Registrable Securities”) of Common Stock at a purchase price of $1.975 per share and accompanying warrant in a private placement for aggregate gross proceeds of approximately $15 million, exclusive of placement agent commission and fees and other offering expenses (the “Offering”). The closing of the Offering is expected to occur on March 10, 2021, subject to the satisfaction of customary closing conditions.\"\n",
    "\n",
    "\n",
    "doc = nlp(test_text)\n",
    "doc = alias_matcher(doc)\n",
    "doc = alias_setter(doc, doc._.secus)\n",
    "# for key, vals in doc._.alias_cache._alias_to_ultimate_origin.items():\n",
    "#     print(doc[key[0]:key[1]], \": \", [doc[v[0]:v[1]] for v in vals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Rework QuantityRelation to be created only for directly related quantity and security\n",
    "add a SECU relation that works like the SourceQuantityRelation but includes both SECUs (source and target)\n",
    "for example in a phrase as:\n",
    "    'As of March 31, 2021, we had outstanding warrants to purchase 7,532,390 shares of our common stock and stock options to purchase 2,176,272 shares of our common stock.'\n",
    "    We should have a SECU which references 'warrants' and one which references 'common stock'\n",
    "    'warrants' SECU should have:\n",
    "        no quantity associcated with it\n",
    "        a datetime relation of March 31, 2021\n",
    "        amods of outstanding\n",
    "        a SECURelation to SECU 'common stock'\n",
    "    first 'common stock' SECU should have:\n",
    "        a QuantityRelation of 7532390 COUNT with a parent_verb 'purchase'\n",
    "        no datetime relation\n",
    "        no amods\n",
    "    'stock options' SECU should have:\n",
    "        a datetime relation of Match 31, 2021\n",
    "        a SECURelation to SECU 'common stock'\n",
    "    second 'common stock' SECU should have:\n",
    "        no datetime relation\n",
    "        a QuantityRelation to 2176272 COUNT with parent verb purchase\n",
    "    \n",
    "    \n",
    "    what we currently have:\n",
    "    a 'warrants' SECU:\n",
    "        QuantityRelations referencing both SECUQUANTITY's belonging to the 'common stock' SECUs\n",
    "        correct datetime_relation\n",
    "    first 'common stock' SECU:\n",
    "        with the datetime relation of March 31, 2021\n",
    "        no QuantityRelation\n",
    "    second 'common stock' SECU:\n",
    "        with the datetimerelation of March 31, 2021\n",
    "        correct QuantityRelation\n",
    "    a 'stock options' SECU:\n",
    "        with the correct datetime_relation\n",
    "        a QuantityRelation belonging to a 'common stock' SECU\n",
    "        wrong amods of [\"common stock\"]\n",
    "    \n",
    "\n",
    "    HOW TO FIX THIS:\n",
    "        1) revisite how SECU objects are created\n",
    "            1.1) create more specific cases for datetime relations\n",
    "            1.2) create more restrictive rules for quantities associated with a SECU\n",
    "        1.5) create a map for QuantityRelations built. I can create the source relation in a second pass\n",
    "            A source relation needs to match on a secuquantity already associated with a none source security through a QuantityRelation\n",
    "\n",
    "        2) create a SECURelation as a bridge between SECU's. get rid of the SourceQuantityRelation\n",
    "        Source -> quantity -> SECU, always? always\n",
    "        OR\n",
    "        the i always need to check the quantity relations source, and just compare the main and source secu to see if they match?\n",
    "    \n",
    "    I should store a relation between two secus based on the parent verb of the QuantityRelation\n",
    "    to get the correct secuquantities from a source secu i need to go up the the dependency tree and take the first none aux verb?\n",
    "    can i start pattern matching from above verb?\n",
    "\n",
    "    verbs to look at:\n",
    "        purchase\n",
    "    \n",
    "Current todo:\n",
    "        work on source_secu_secuquantity patterns [x]\n",
    "        remove old source quantity relation setting [x]\n",
    "        add new source quantity relation and context getting in SECUObjectMapper [x]\n",
    "\n",
    "        reimplement the extractors according to the changes which \n",
    "            come with adding the SECU class\n",
    "            + add securities_accn_occurence table into equation so we can keep track were we found security \n",
    "            -> adjust handlers, commands and add model object [x]\n",
    "            1) test current state of model and databse [x]\n",
    "            2) see if Security._security_attributes_object can be accessed after creation of Security and that it isnt present when Security is queried from database\n",
    "            -> workaround used, converting json to dict when needed [x]\n",
    "            3) implement Company.get_security_by_attributes [x]\n",
    "            4) implement handlers, command for SecurityAccnOccurence [x]\n",
    "            5) write tests for 4) [x]\n",
    "            \n",
    "        rework tests for changes above  [x]\n",
    "        implement outstanding securities into pipeline:\n",
    "        1) filing_nlp (get outstanding from SECU) [x]\n",
    "        2) command for security fact outstanding [x]\n",
    "        3) handler for add_outstanding_securities [x]\n",
    "        4) BaseHTMExtractor function to form correct model objects to pass to handler [x]\n",
    "        issues:\n",
    "            * how can i fetch the correct model.Security from a SECU object\n",
    "            * is the best way to also cram this onto the doc -> no because we add too much coupling between filing_nlp and model\n",
    "            --> went for wrapper class with a map of model.Security->SECU\n",
    "        5) remove old BaseHTMExtractor [x]\n",
    "        6) run extractor tests and rewrite as needed [x]\n",
    "        7) fix DependencyAttributeMatcher tests, some issue with hashability and DatetimeRelation [x]\n",
    "        run all tests [x]\n",
    "\n",
    "        8) rework agreement matcher:\n",
    "            *) implement a universal way for aliases in a seperat component\n",
    "            Should the target for a alias be a span or could it be a single token?\n",
    "            How would i determine what the targets for the aliases are?\n",
    "            -> pass in list[Token|Span]\n",
    "            -> write a fetch for Span and a fetch for Token\n",
    "            \n",
    "            FROM SPAN TO ALIAS (disallowing spans across sentence boundaries):\n",
    "            0) create a inter component map for already assigned aliases by index (assigned_aliases) [x]\n",
    "            1) get all aliases [x]\n",
    "            2) create an alias to sent map [x]\n",
    "            4) pass spans were we want to know if they have an alias [x]\n",
    "            5) fetch aliases of span.sent and t.i > span.i and not yet assigned [x]\n",
    "            6) get similarity_score [x]\n",
    "\n",
    "            --\n",
    "            1) test if AliasSetter works as good as current SECU alias setting\n",
    "            2) add extension attribute to identify base alias and reference to alias \n",
    "            3) rework SpacyFilingTextSearch to use AliasSetter\n",
    "                *) new SECUMatcher __call__ [x]\n",
    "                    - call AliasMatcher\n",
    "                    - matcher_SECU(doc)\n",
    "                    - classify refs and base aliases as SECU aswell if they have a SECU as origin -> handeling special SECU cases\n",
    "                    - retokenize\n",
    "                    - reinititalize_extensions of AliasMatcher\n",
    "                    - call AliasMatcher\n",
    "                    - call AliasSetter\n",
    "                    - update_doc_secus_spans\n",
    "                    - matcher_SECUREF\n",
    "                    - matcher_SECUATTR\n",
    "                *) Remove all references to old unused extensions of SECUMatcher \n",
    "                   and replace with new ones from AliasMatcher [x]\n",
    "                *) rework secu_key to be set the same for base and reference aliases\n",
    "                   as for their origin\n",
    "        \n",
    "        9) make AliasSetter and AliasMatcher be passed as a config\n",
    "           parameter to components which need it [x]\n",
    "        10) rework old code of dilution_scout and sec_scraping to allow\n",
    "            for change from old way of using outstanding shares (separate table, which was only for common stock)\n",
    "            to the new way (where we actually create a distinct security for the common stock and add the outstanding with reference to it)\n",
    "            1) rework code of companyfacts population [x]\n",
    "                flow we currently have:\n",
    "                    - on update_ticker() or in init_pop()\n",
    "                    - update_outstanding_shares() -> rename this to include common stock\n",
    "                    -> _update_oustanding_shares_base_on_companyfacts()\n",
    "                    -> get_outstanding_shares()\n",
    "                    -> write directly to the outstanding_shares table\n",
    "                flow we want:\n",
    "                    - on update_ticker() (since we want to call this from init_pop anyway, after creating necessary entries)\n",
    "                    - update_outstanding_common_stock_based_on_companyfacts()\n",
    "                    -> get_outstanding_common_stock_from_companyfacts()\n",
    "                    -> ensure we have a security which is in list of valid common stock names\n",
    "                    -> add entries through bus to this security\n",
    "                other changes that need to be made:\n",
    "                    - check if we have a common stock security otherwise issue a command to create a default one [x]\n",
    "            \n",
    "            1.5) adjust constructors for AgreementMatcher and SECUMatcher for Alias change [x]\n",
    "            2) rework code inside of dilution_scout for fetch [x]\n",
    "                *) add fetch for all securities [x]\n",
    "            2.5) change fetch calls and adjust formatting of result to be object[security_info, list[datapoint]] [x]\n",
    "            3) rework code inside of dilution_scout for presentation\n",
    "                *) fix resize of charts and zoom in general //think i found the issue in minwidth media 1000px @shtylesheet.css\n",
    "               (dropdowns with a graph for each security that has outstanding shares)\n",
    "                3.1) create a dummy version where we create a graph for every item in the list [x]\n",
    "                3.2) add foldable dropdown with headers for each security [x]\n",
    "                3.3) group by common, preferred, warrants, debt ect\n",
    "                3.4) [maybe] exclude securities where none are outstanding as of the query date \n",
    "                ------\n",
    "                Add Context from 11) first so we can add more content at once and have a better\n",
    "                representation of the actual outstanding securties besides the common stock\n",
    "                then continue with subpoint 4)\n",
    "                4) rollout and repop entire database\n",
    "                    4.1) backup db\n",
    "                    4.2) delete\n",
    "                    4.3) run pop\n",
    "                    4.4) test pop\n",
    "                5) write query to fetch all offerings\n",
    "                6) work on finding increase of authorized shares through 8k item 5.03/5.07\n",
    "\n",
    "        11) Create a general solution for a origin -> references relationship.\n",
    "            This would be used to create a tree of entities referencing the same thing,\n",
    "            but they can have different textual representations. This would make it easier\n",
    "            to add something like coreferee later one.\n",
    "            lets look at an example and take some notes:\n",
    "\n",
    "                \"On March 8, 2021, Hoth Therapeutics, Inc. (the “Company” ) entered into a\n",
    "                securities purchase agreement (the “Purchase Agreement”) with certain \n",
    "                institutional and accredited investors (the “Investors”) pursuant to which \n",
    "                it agreed to sell an aggregate of \n",
    "                (i) 6,826,962 shares (the “Shares”) of common stock, \n",
    "                par value $0.0001 per share (the “Common Stock”), \n",
    "                (ii) warrants (the “Pre-Funded Warrants”) to purchase \n",
    "                up to 767,975 shares (the “Pre-funded Warrant Shares”) of Common Stock and \n",
    "                (iii) warrants (the “Common Stock Warrants” and together with the Pre-Funded Warrants, the “Warrants”) \n",
    "                to purchase up to 7,594,937 shares \n",
    "                (the “Warrant Shares” and together with the Shares and the Pre-Funded Warrant Shares, the “Registrable Securities”)\n",
    "                of Common Stock at a purchase price of $1.975 per share and accompanying \n",
    "                warrant in a private placement for aggregate gross proceeds of \n",
    "                approximately $15 million, exclusive of placement agent commission and \n",
    "                fees and other offering expenses (the “Offering”).\n",
    "                \n",
    "                The closing of the Offering is expected to occur on March 10, 2021, \n",
    "                subject to the satisfaction of customary closing conditions.\"\n",
    "\n",
    "            issues to solve:\n",
    "                1) base_aliases created as reference to multiple base_aliases eg: \n",
    "                    (the “Common Stock Warrants” and together with the Pre-Funded Warrants, the “Warrants”)\n",
    "                2) how to identify the accompanying correct base and therefor establish the right quantity relation\n",
    "                   for further disection of a security referenced in a context as described in 1)\n",
    "                3) how can i account for or rather ignore aliases disrupting patterns\n",
    "                   like the disruption caused in (i) and (ii) in the above text\n",
    "                4) types of relations to consider:\n",
    "                   - origin -> reference relations\n",
    "                   - base_alias or/and reference_alias creating a new origin for a base_alias \n",
    "                        (so basically a graph where multiple branches combine into a new one)\n",
    "                     -> which gives us [base_aliases + reference_aliases + origin]-> base_alias relation\n",
    "                     (+variations omitting two, or one of three, + classes repeating with unique items)\n",
    "                    they could be represented as: \n",
    "                        *)\n",
    "                    what is currently implemented:\n",
    "                        - origin -> base\n",
    "                        - base -> [references]\n",
    "                    what i would need:\n",
    "                        - (origin | base | reference) -> base\n",
    "                          where base and reference in assignment can be traced back to their origins\n",
    "                        - (origin | base | reference) -> (origin | base | reference)\n",
    "                        1) parent child map with references to the ultimate origin, \n",
    "                            map for origin to references (any type),\n",
    "                            a map of reference to type and \n",
    "                            a map of type to set of occurence\n",
    "                            a map of type to list of occurence ordered by start idx\n",
    "                            a set of children making up multi aliases\n",
    "                        3) change __call__ of AliasMatcher to include \"... and together\" and \"collectively..\" pattern\n",
    "                         *) simplify and only take the first alias in a parantheses_match as possible candidate to be a\n",
    "                            a regular base_alias which can be assigned in the similarity_score way\n",
    "                        4) \n",
    "\n",
    "                5) ban list for aliases like \"Shares\" in above text ?\n",
    "                6) plurals of aliases:\n",
    "                    sometimes present as: The foregoing persons are hereinafter sometimes individually referred to as a “Reporting Person” and collectively referred to as the “Reporting Persons”. \n",
    "                    otherwise?\n",
    "                7) aliases not present as parenthesis pattern\n",
    "\n",
    "\n",
    "        11) Do I need a referenceCache ? (to tie together aliases and other types of references)\n",
    "            NOT SURE YET, REVISIT AFTER WE GOT CONTEXT SCOPE DOWN\n",
    "            allowed to use coreferee? [yes] (MIT license, checked on 23.01.2023)\n",
    "            --> just feed coref into aliasCache and give a different type\n",
    "        \n",
    "\n",
    "        11.5) profile AliasSetter, CorefSetter, AliasCache for faulty logic [x]\n",
    "        -> thing that makes it 30% slower is coreferee \n",
    "\n",
    "        11.6) add \"units\" as SECU entity\n",
    "     \n",
    "        CURRENTLY WORKING ON ITEM: 12 <---------------------------------------------------------------------------------------------\n",
    "        12) think of how to implement the contractual relation of a SECU object\n",
    "            1) start by simply extracting the sentence containing the CONTRACT entity and check for rules.\n",
    "            2) should probably not do 1) and instead focus on sentences with a CONTRACT ent and a SECU-object\n",
    "               with a exercise price attached.\n",
    "            A Contract Context is defined by:\n",
    "                - Subjects (parties entering into the contract)\n",
    "                    *) will need to add more precise tagging of ORG for companies \n",
    "                      (could work with a simple list of all company names \n",
    "                      in lower and regex it, then train model for COMPANY ent to extend it?)\n",
    "                       1) get all company through ticker:name:cik mapping from SEC website\n",
    "                       2) create list to feed into a matcher/phrase matcher matching on .lower_\n",
    "                       3) ?\n",
    "                    *) need to create a seperate component for that, and add the\n",
    "                       most common aliases for commpanies\n",
    "                    see filing_nlp_constants.py (COMMON_CONTRACT_SUBJECTS) for some common entities\n",
    "                - Scope (What sentences or parts thereof are covered)\n",
    "                 1) assume any other NamedEntity linked by the first none aux verb connected to the contract entity\n",
    "                    is in scope\n",
    "                 2) \n",
    "                - Obligations/Limitations (What do the parties need to adhere to)\n",
    "                - Actions made through contract (eg: pursuant to contract X we issued Y shares of Z)\n",
    "                - Start/Duration of contract\n",
    "                \n",
    "            1) we create a Contract class similar to the SECU objects\n",
    "               we keep the references strictly to tuples of start, end indices\n",
    "\n",
    "            *) I could create a graph for origin and contexts asssociated with them or/and\n",
    "               I could make a contract_relation attribute on the SECU object\n",
    "               \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        other todo:\n",
    "        *) rework uow for company to get by cik (or by symbol, but through getting cik and then getting company with gotten cik)\n",
    "        *) spin of filing_nlp of parser folder into its own project\n",
    "        *) add tests for SecurityDependencyAttributeMatcher\n",
    "        *) Think about how to Scope context from a given entity\n",
    "            coreferee?\n",
    "            1) how can i establish context across sentence boundary\n",
    "            2) what are the contexts i really want?\n",
    "                for securities:\n",
    "                    - time context\n",
    "                    - contractual context\n",
    "                    -\n",
    "                \n",
    "        *) add context for prep contexts to verbs (at/through)\n",
    "\n",
    "        Backburner:\n",
    "        1) rename .doc attribute on Filing classes so it doesnt create confusion with spacy Doc\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 ('sec_scraping_testing-er4NC4pi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6071678dbb69e107a06c616957bae4c441d4d322577d1f6af58eab6659c9cd7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
